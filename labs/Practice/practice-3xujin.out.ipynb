{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#"
      ],
      "id": "70787d89-f217-48d3-834e-80904740e079"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap"
      ],
      "id": "cell-0"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "pd.read_csv(\n",
        "    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n",
        ")"
      ],
      "id": "cell-1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"><head>\\n\\n<meta charset=\"utf-8\">\\n<meta name=\"generator\" content=\"quarto-1.5.56\">\\n\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\">\\n\\n<meta name=\"author\" content=\"Arthur Turrell\">\\n'"
            ]
          }
        }
      ],
      "source": [
        "url = \"http://aeturrell.com/research\"\n",
        "page = requests.get(url)\n",
        "page.text[:300]"
      ],
      "id": "cell-2"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       </div>\n",
            "          <div class=\"project-category\">\n",
            "           <a href=\"#category=gender pay gap\">\n",
            "            gender pay gap\n",
            "           </a>\n",
            "          </div>\n",
            "          <div class=\"project-category\">\n",
            "           <a href=\"#category=labour\">\n",
            "            labour\n",
            "           </a>\n",
            "          </div>\n",
            "          <div class=\"project-category\">\n",
            "           <a href=\"#category=text analysis\">\n",
            "            text analysis\n",
            "           </a>\n",
            "          </div>\n",
            "         </div>\n",
            "         <div class=\"project-details-listing"
          ]
        }
      ],
      "source": [
        "soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "print(soup.prettify()[60000:60500])"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "<p>Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" <i>Environment and Planning B: Urban Analytics and City Science</i> (2024): 23998083241267331. doi: <a href=\"https://doi.org/10.1177/23998083241267331\"><code>10.1177/23998083241267331</code></a></p>"
            ]
          }
        }
      ],
      "source": [
        "# Get all paragraphs\n",
        "all_paras = soup.find_all(\"p\")\n",
        "# Just show one of the paras\n",
        "all_paras[1]"
      ],
      "id": "cell-4"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'"
            ]
          }
        }
      ],
      "source": [
        "all_paras[1].text"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n",
              " 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n",
              " 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n",
              " 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']"
            ]
          }
        }
      ],
      "source": [
        "projects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\n",
        "projects = [x.text.strip() for x in projects]\n",
        "projects[:4]"
      ],
      "id": "cell-6"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "df_list = pd.read_html(\n",
        "    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n",
        ")\n",
        "# Retrieve first and only entry from list of dataframes\n",
        "df = df_list[0]\n",
        "df.head()"
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting pdftotext\n",
            "  Downloading pdftotext-3.0.0.tar.gz (113 kB)\n",
            "     ------------------------------------ 113.6/113.6 kB 264.9 kB/s eta 0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: still running...\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: pdftotext\n",
            "  Building wheel for pdftotext (pyproject.toml): started\n",
            "  Building wheel for pdftotext (pyproject.toml): finished with status 'error'\n",
            "Failed to build pdftotext\n",
            "Note: you may need to restart the kernel to use updated packages."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for pdftotext (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [9 lines of output]\n",
            "      WARNING: pkg-config not found--guessing at poppler version.\n",
            "               If the build fails, install pkg-config and try again.\n",
            "      WARNING: pkg-config not found--guessing at poppler version.\n",
            "               If the build fails, install pkg-config and try again.\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_ext\n",
            "      building 'pdftotext' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for pdftotext\n",
            "ERROR: Could not build wheels for pdftotext, which is required to install pyproject.toml-based projects"
          ]
        }
      ],
      "source": [
        "%pip install pdftotext"
      ],
      "id": "cell-8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Downloading imdb top 250 movie's data\n",
        "url = 'http://www.imdb.com/chart/top'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")"
      ],
      "id": "cell-10"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "movies = soup.select('td.titleColumn')\n",
        "crew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\n",
        "ratings = [b.attrs.get('data-value')\n",
        "        for b in soup.select('td.posterColumn span[name=ir]')]"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a empty list for storing\n",
        "# movie information\n",
        "list = []\n",
        "\n",
        "# Iterating over movies to extract\n",
        "# each movie's details\n",
        "for index in range(0, len(movies)):\n",
        "    \n",
        "    # Separating movie into: 'place',\n",
        "    # 'title', 'year'\n",
        "    movie_string = movies[index].get_text()\n",
        "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
        "    movie_title = movie[len(str(index))+1:-7]\n",
        "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
        "    place = movie[:len(str(index))-(len(movie))]\n",
        "    data = {\"place\": place,\n",
        "            \"movie_title\": movie_title,\n",
        "            \"rating\": ratings[index],\n",
        "            \"year\": year,\n",
        "            \"star_cast\": crew[index],\n",
        "            }\n",
        "    list.append(data)"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "for movie in list:\n",
        "    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
        "        ') -', 'Starring:', movie['star_cast'], movie['rating'])"
      ],
      "id": "cell-13"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "#saving the list as dataframe\n",
        "#then converting into .csv file\n",
        "df = pd.DataFrame(list)\n",
        "df.to_csv('imdb_top_250_movies.csv',index=False)"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#saving the list as dataframe\n",
        "#then converting into .csv file\n",
        "df = pd.DataFrame(list)\n",
        "df.to_csv('imdb_top_250_movies.csv',index=False)"
      ],
      "id": "cell-15"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
            "Note: you may need to restart the kernel to use updated packages."
          ]
        }
      ],
      "source": [
        "%pip install requests"
      ],
      "id": "cell-16"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: beautifulsoup4 in d:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
            "Note: you may need to restart the kernel to use updated packages."
          ]
        }
      ],
      "source": [
        "%pip install beautifulsoup4"
      ],
      "id": "cell-17"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        " \n",
        "# 定义请求的 URL 和 headers\n",
        "url = \"https://movie.douban.com/top250\"\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}"
      ],
      "id": "cell-18"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "id": "cell-19"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        " \n",
        "# 将数据保存到 CSV 文件\n",
        "with open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['title', 'description', 'rating', 'votes']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        " \n",
        "    writer.writeheader()  # 写入表头\n",
        "    for movie in movies:\n",
        "        writer.writerow(movie)  # 写入每一行数据"
      ],
      "id": "cell-20"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        " \n",
        "# 将数据保存到 CSV 文件\n",
        "with open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['title', 'description', 'rating', 'votes']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        " \n",
        "    writer.writeheader()  # 写入表头\n",
        "    for movie in movies:\n",
        "        writer.writerow(movie)  # 写入每一行数据"
      ],
      "id": "cell-21"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "base",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  }
}